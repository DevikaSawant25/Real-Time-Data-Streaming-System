âš¡ Real-Time Data Streaming System

Tech Stack: Apache Kafka | Spark Structured Streaming | PostgreSQL | Python | Docker

ğŸ§  Overview

This project demonstrates a real-time clickstream data pipeline using
Apache Kafka and Spark Structured Streaming, with PostgreSQL as the analytical data store.
The system continuously ingests simulated user events from a Python producer,
processes them in real time with Spark, and writes both raw events and aggregated metrics into PostgreSQL for analytics dashboards.

Highlights

Sub-second stream ingestion and transformation
Real-time data aggregation and persistence
Fully containerized setup with Docker Compose
Fault-tolerant and replayable Kafka topic design

ğŸ§© Architecture
flowchart LR
    A[Python Producer] -->|JSON Events| B[(Kafka Topic: clickstream)]
    B -->|Structured Streaming| C[Spark Streaming Job]
    C -->|Write Raw Events| D[(PostgreSQL - clickstream_events)]
    C -->|Aggregate 1-min Windows| E[(PostgreSQL - clickstream_agg_minute)]
    E --> F[PowerBI / Grafana Analytics]

âš™ï¸ Core Features

âœ… Real-time Ingestion: Simulated user activity streamed via Kafka Producer
âœ… Event Processing: Spark Structured Streaming for ETL & window aggregation
âœ… Data Persistence: PostgreSQL for storing both raw and aggregated data
âœ… Streaming Analytics: Minute-level traffic metrics for URLs and sessions
âœ… Dockerized Infrastructure: Zookeeper, Kafka, Spark, Postgres, Adminer all in one stack

ğŸ§± Pipeline Components
Component	Technology	Description
Data Producer	Python + kafka-python	Generates synthetic clickstream JSON events
Message Broker	Apache Kafka	Ensures distributed, reliable event streaming
Stream Processor	Apache Spark Structured Streaming	Parses, cleans, aggregates events in micro-batches
Database Sink	PostgreSQL	Stores processed raw and aggregated metrics
Visualization	PowerBI / Grafana	Displays real-time analytics dashboards
ğŸ§® Data Model
ğŸŸ£ Raw Table â€“ clickstream_events
event_id	user_id	event_ts	url	referrer	ip	received_at
d7f3-91a2...	201	2025-10-25 09:12:10	/product/101	google.com	192.168.1.45	2025-10-25 09:12:11
ğŸŸ¢ Aggregated Table â€“ clickstream_agg_minute
window_start	window_end	url	hits
2025-10-25 09:10:00	2025-10-25 09:11:00	/product/101	142
ğŸ§° Setup Instructions
1ï¸âƒ£ Clone & Configure
git clone https://github.com/DevikaSawant25/real-time-data-streaming.git
cd real-time-data-streaming
cp .env.example .env

2ï¸âƒ£ Start the Services
docker compose up -d zookeeper kafka postgres adminer
docker compose up -d producer spark

3ï¸âƒ£ Monitor in Adminer

Open http://localhost:8081
System: PostgreSQL
Server: postgres
User: postgres
Password: postgres
Database: analytics

ğŸ§ª Spark Processing Logic

Reads Kafka topic: clickstream
Parses JSON schema â†’ validates fields
Cleans & deduplicates records by event_id
Writes results using foreachBatch() to PostgreSQL
Performs 1-minute aggregations with watermarks to handle late data

ğŸ“ˆ Performance & Results

Achieved sub-second latency between ingestion and availability in PostgreSQL
Processed over 5K events/minute on a local setup
Delivered real-time visibility into product and page traffic trends

ğŸ“¦ Project Structure
rt-streaming/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â””â”€â”€ producer.py
â”‚   â””â”€â”€ spark_streaming_job.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run.sh
â”‚   â””â”€â”€ stop.sh
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

ğŸ§  Key Learnings

Event-driven architectures with Kafka topics and partitions
Real-time data ETL with Spark Structured Streaming
Writing streaming data to relational databases safely
End-to-end orchestration and containerization using Docker

ğŸ Future Enhancements

Integrate Prometheus + Grafana for stream monitoring
Add Flink / Kafka Streams comparison module
Deploy on AWS MSK + EMR + RDS for production scaling
